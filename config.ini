# We will use this config to make iterating the training much easier,
# you can find all the relevant tunable parameters for the project 

[testing]
testing = 0
data_samples_for_testing = 1000

[callbacks]
neptune_logger = 0
neptune_project = ""
neptune_token = ""
upload_model = 1
model_checkpoint = 1
model_checkpoint_type = 'accuracy' # accuracy or loss
model_checkpoint_verbose = 1
remove_previous_checkpoint_at_start = 1
hyperopt = 0

[paths]
model_name = "model.pth"
model_path = "models/"
labeled = "data/train_audio"
metadata = "data/train_metadata.csv"
model_checkpoint_path = "models/model_checkpoint/"

[trainer]
seed = 42
batch_size_train = 8
batch_size_val = 8
batch_size_test = 8
num_workers = 2
n_epochs = 50
load_best_at_end = 1

[agent]
lr_decay = 1 
lr_decay_type = "warmup_cos" # lin, exp, cos, warmup_cos
lr_start = 0.00001
lr_warmup_end = 0.0005
lr_end = 0.0002
warmup_epochs = 5
exp_gamma = 0.98945
lr_verbose = 0
loss = "focal_loss" # cross_entropy or focal_loss
optimizer = "adam" # sgd, adam, rmsprop, adamw

[model]
type = "efficientnet_v2_m" # mobilenet_v3_small, mobilenet_v3_large, efficientnet_v2_s, efficientnet_v2_m, efficientnet_v2_l - might not work, resnet50
transfer_learning = 1

[data]
hash_path = "processed_data/"
train_ratio = 0.8
test_val_ratio = 0.5
min_samples_in_class = 10
shuffle = 1
output_dir = "processed_data/"
num_workers = 2
multi_threading = 1

[data_process]
sample_rate = 32000
n_mels = 224
n_fft = 2048
hop_length = 512
max_length_s = 15
f_max = 16000
f_min = 20
use_librosa = 1
mode = 'slice' # slice or single
pad_mode = 'center' # center or end
pad_values = 'zeros' # repeat or zeros
resize_method = "manual" # 'function_resize', 'manual'
standardise = 0
normalise = 0
change_this_to_reprocess = 0

[augmentation]
data_augmentation = 0
augment_add_noise = 1
noise_level = 0.005
augment_spec_augment = 1
