# We will use this config to make iterating the training much easier,
# you can find all the relevant tunable parameters for the project 

[testing]
testing = 0
data_samples_for_testing = 2500

[callbacks]
neptune_logger = 1
neptune_project = "BirdClefHomework/csipcsirip"
neptune_token = ""
upload_model = 1
model_checkpoint = 1
model_checkpoint_type = 'loss' # accuracy or loss
model_checkpoint_verbose = 1
remove_previous_checkpoint_at_start = 1

[paths]
model_name = "model.pth"
model_path = "models/"
labeled = "data/train_audio"
metadata = "data/train_metadata.csv"
model_checkpoint_path = "models/model_checkpoint/"

[trainer]
seed = 42
batch_size_train = 16
batch_size_val = 10
batch_size_test = 10
num_workers = 6
n_epochs = 100

[agent]
lr_decay = 1 
lr_decay_type = "cos" # lin, exp, cos, warmupcosine
lr_start = 0.01
lr_warmup_end = 0.01
lr_end = 0.00005
exp_gamma = 0.97
lr_verbose = 0
loss = "focal_loss" # cross_entropy or focal_loss
optimizer = "adam" # sgd, adam, rmsprop

[model]
type = "mobilenet_v3_small" # mobilenet_v3_small, mobilenet_v3_large, efficientnet_v2_s, efficientnet_v2_m, efficientnet_v2_l - might not work, resnet50
transfer_learning = 1

[data]
hash_path = "processed_data/"
train_ratio = 0.8
test_val_ratio = 0.5
min_samples_in_class = 100
shuffle = 1
output_dir = "processed_data/"
num_workers = 14
multi_threading = 1

[data_process]
sample_rate = 32000
n_mels = 128
n_fft = 1024
hop_length = 256
max_length_s = 15
f_max = 16000
f_min = 512
mode = 'single' # slice or single
pad_mode = 'center' # center or end
pad_values = 'zeros' # repeat or zeros
standardise = 1
normalise = 1
change_this_to_reprocess = 21113

[augmentation]
data_augmentation = 1
augment_add_noise = 1
noise_level = 0.005
augment_spec_augment = 1
