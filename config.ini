# We will use this config to make iterating the training much easier,
# you can find all the relevant tunable parameters for the project 

[callbacks]
neptune_logger = 0
neptune_project = "BirdClefHomework/csipcsirip"
neptune_token = ""
model_checkpoint = 0
model_checkpoint_type = 'accuracy' # 'accuracy', 'loss'
model_checkpoint_verbose = 1 

[paths]
model_name = "model.pth"
model_path = "models/"
labeled = "data/train_audio"
metadata = "data/train_metadata.csv"
model_checkpoint_path = "models/model_checkpoint/checkpoint.pth"

[trainer]
seed = 0
batch_size_train = 1024
batch_size_val = 1024
batch_size_test = 1024
num_workers = 6
n_epochs = 300

[agent]
lr_decay = 1 
lr_decay_type = "exp" # lin, exp, cos, warmupcosine
lr_start = 0.0001
lr_warmup_end = 0.01
lr_end = 0.00005
exp_gamma = 0.97
lr_verbose = 12
loss = "focal_loss" # cross_entropy, focal_loss
optimizer = "adam" # sgd, adam, rmsprop

[model]
type = "mobilenet_v3_small" # A small model we already know, it's far from final
transfer_learning = 0

[data]
data_num_for_testing = 50
train_ratio = 0.8
test_val_ratio = 0.5
min_samples_in_class = 10
shuffle = 1
data_augmentation = 0
