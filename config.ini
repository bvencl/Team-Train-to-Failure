# We will use this config to make iterating the training much easier,
# you can find all the relevant tunable parameters for the project 

[testing]
testing = 1
data_samples_for_testing = 2500

[callbacks]
neptune_logger = 0
neptune_project = "BirdClefHomework/csipcsirip"
neptune_token = "eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJkZGI5ZDY0Ni03MTEyLTQ2N2UtOTRlMC0zYzBlZmI1ODAxMTkifQ=="
model_checkpoint = 1
model_checkpoint_type = 'accuracy' # accuracy or loss
model_checkpoint_verbose = 0
remove_previous_checkpoint_at_start = 1

[paths]
model_name = "model.pth"
model_path = "models/"
labeled = "data/train_audio"
metadata = "data/train_metadata.csv"
model_checkpoint_path = "models/model_checkpoint/"

[trainer]
seed = 42
batch_size_train = 10
batch_size_val = 10
batch_size_test = 10
num_workers = 6
n_epochs = 10

[agent]
lr_decay = 1 
lr_decay_type = "cos" # lin, exp, cos, warmupcosine
lr_start = 0.01
lr_warmup_end = 0.01
lr_end = 0.00005
exp_gamma = 0.97
lr_verbose = 0
loss = "cross_entropy" # cross_entropy or focal_loss
optimizer = "adam" # sgd, adam, rmsprop

[model]
type = "resnet50" # mobilenet_v3_small, mobilenet_v3_large, efficientnet_v2_s, efficientnet_v2_m, efficientnet_v2_l - might not work, resnet50
transfer_learning = 1

[data]
hash_path = "processed_data/"
train_ratio = 0.8
test_val_ratio = 0.5
min_samples_in_class = 150
shuffle = 1
output_dir = "processed_data/"
num_workers = 14
multi_threading = 0

[data_process]
sample_rate = 32000
n_mels = 256
n_fft = 2048
hop_length = 512
max_length_s = 15
f_max = 16000
f_min = 20
mode = 'single' # slice or single
pad_mode = 'end' # center or end
pad_values = 'repeat' # repeat or zeros
change_this_to_reprocess = 32 

[augmentation]
data_augmentation = 1
augment_add_noise = 0
augment_spec_augment = 1
